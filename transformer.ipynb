{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9155e348-1785-4579-82b6-1bb8f88f06d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "be1e671c-5d9e-4369-bbaa-087e54e35343",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_embedding_size: int, kq_embedding_size: int, v_embedding_size: int, masked: bool = False):\n",
    "        super().__init__()\n",
    "        # Input Shape=(T, S) \n",
    "        \n",
    "        # Shape=(S, KQ)\n",
    "        self.K_w = torch.nn.Linear(input_embedding_size, kq_embedding_size)\n",
    "        # Shape=(S, KQ)\n",
    "        self.Q_w = torch.nn.Linear(input_embedding_size, kq_embedding_size)\n",
    "\n",
    "        # Shape=(S,V)\n",
    "        self.V_w = torch.nn.Linear(input_embedding_size, v_embedding_size)\n",
    "        self.V_scale = 1.0 / np.sqrt(v_embedding_size)\n",
    "\n",
    "        # if true, hides future values (decoder), otherwise allows peeking forward\n",
    "        # in time (encoder)\n",
    "        self._masked = masked\n",
    "\n",
    "    def forward(self, input_sequence):\n",
    "        # (T, S) x (S, KQ) -> (T, KQ)\n",
    "        queries = self.Q_w(input_sequence)\n",
    "        # (T, S) x (S, KQ) -> (T, KQ)\n",
    "        keys = self.K_w(input_sequence)\n",
    "\n",
    "        # (T, KQ) X (KQ, T) -> (T, T)\n",
    "        attention_matrix = self.V_scale * (queries @ torch.transpose(keys, 0, 1))\n",
    "\n",
    "        # Block out future values for each item in the sequence.\n",
    "        # -infinity goes to 0 in the softmax.\n",
    "        if self._masked:\n",
    "            upper_mask = torch.ones_like(attention_matrix).tril() == 0\n",
    "            attention_matrix = attention_matrix.masked_fill(upper_mask, -np.inf)\n",
    "        # Normalizes row-by-row. (T, T)\n",
    "        normalized_attention_matrix = torch.softmax(attention_matrix, dim=1)\n",
    "        print('weights', normalized_attention_matrix)\n",
    "\n",
    "        # (T, S) x (S, V) -> (T, V)\n",
    "        values =  self.V_w(input_sequence)\n",
    "\n",
    "        # (T, T) x (T, V) = (T, V)\n",
    "        return normalized_attention_matrix @ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9d229cb5-b13a-498b-96cd-b4acf5accb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadedAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads: int, input_embedding_size: int, kq_embedding_size: int, v_embedding_size: int, masked:bool=False):\n",
    "        super().__init__()\n",
    "        # TODO: only final embedding size should be specified. we should divide this among the heads\n",
    "        self.head_stack = [Attention(input_embedding_size, kq_embedding_size, v_embedding_size, masked=masked) for _ in range(n_heads)]\n",
    "\n",
    "    def forward(self, input_sequence):\n",
    "        return torch.hstack([head(input_sequence) for head in self.head_stack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7cb4e2b0-1648-4355-aacb-403dc1812c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2000, 0.8000],\n",
       "        [0.5000, 0.5000]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensor = torch.tensor([[0.2, 0.8], [0.5, 0.5]])\n",
    "test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d633b9fe-eb77-42ee-95ac-a0eda156232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "att = Attention(input_embedding_size=2, kq_embedding_size=5, v_embedding_size=10, masked=True)\n",
    "mha = MultiheadedAttention(n_heads =5, input_embedding_size=2, kq_embedding_size=5, v_embedding_size=10, masked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6faa429b-2ab7-439f-b171-627eee52385c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights tensor([[0.5128, 0.4872],\n",
      "        [0.5151, 0.4849]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8743,  0.0041,  0.5484,  0.2443,  0.5566, -0.8311, -0.2982, -0.5566,\n",
       "         -0.7567,  0.0167],\n",
       "        [-0.8741,  0.0047,  0.5481,  0.2437,  0.5573, -0.8311, -0.2980, -0.5570,\n",
       "         -0.7573,  0.0169]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att = Attention(input_embedding_size=2, kq_embedding_size=5, v_embedding_size=10, masked=False)\n",
    "att.forward(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0a42d6c3-2e5d-4f28-8897-5f4ee67da6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights tensor([[1.0000, 0.0000],\n",
      "        [0.4947, 0.5053]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1343,  0.7997, -0.9043,  0.3885,  0.5285,  0.4651,  0.1492,  0.3160,\n",
       "         -0.3017, -0.0447],\n",
       "        [ 0.1696,  0.7208, -0.7852,  0.3348,  0.4258,  0.5256,  0.0345,  0.3818,\n",
       "         -0.2358, -0.0599]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_att = Attention(input_embedding_size=2, kq_embedding_size=5, v_embedding_size=10, masked=True)\n",
    "masked_att.forward(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26327b8c-ee3d-4749-a89e-a0df2a652e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8862f0c6-4c2d-430f-a1ae-63bf1778e1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights tensor([[1.0000, 0.0000],\n",
      "        [0.5130, 0.4870]], grad_fn=<SoftmaxBackward0>)\n",
      "weights tensor([[1.0000, 0.0000],\n",
      "        [0.4910, 0.5090]], grad_fn=<SoftmaxBackward0>)\n",
      "weights tensor([[1.0000, 0.0000],\n",
      "        [0.4915, 0.5085]], grad_fn=<SoftmaxBackward0>)\n",
      "weights tensor([[1.0000, 0.0000],\n",
      "        [0.5191, 0.4809]], grad_fn=<SoftmaxBackward0>)\n",
      "weights tensor([[1.0000, 0.0000],\n",
      "        [0.5166, 0.4834]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0493, -0.3194, -0.3123,  0.2131, -0.8150,  0.4920,  0.3484,  0.5023,\n",
       "         -0.7059, -0.4539,  0.7817, -1.1859, -0.5948,  0.2744, -0.2611, -0.7955,\n",
       "         -0.6004, -0.6219,  0.2608, -0.6604,  0.3387,  0.0361,  0.3525, -0.6725,\n",
       "         -0.2645, -0.5994, -0.0470, -0.4691,  0.1977,  0.1626, -0.8820, -0.1713,\n",
       "          0.1643, -0.4443, -0.0565,  0.8806, -0.8181, -0.4663,  0.0050,  0.9629,\n",
       "         -0.8639, -0.6341,  0.5698,  0.2015,  0.7313,  0.3077,  0.5483, -0.4744,\n",
       "          0.2041, -0.1124],\n",
       "        [ 0.2301, -0.2138, -0.2308,  0.1769, -0.6883,  0.5825,  0.3498,  0.4796,\n",
       "         -0.6806, -0.4117,  0.6675, -1.1405, -0.6076,  0.3759, -0.2764, -0.7715,\n",
       "         -0.4457, -0.5670,  0.4318, -0.5113,  0.1845, -0.0296,  0.3962, -0.5674,\n",
       "         -0.1871, -0.7022,  0.0090, -0.3089,  0.2506, -0.0073, -0.8790, -0.1680,\n",
       "          0.1970, -0.3761, -0.2002,  0.8629, -0.7113, -0.4607,  0.0389,  0.9654,\n",
       "         -0.7377, -0.4755,  0.4311,  0.1662,  0.8252,  0.3023,  0.4956, -0.4705,\n",
       "          0.0812, -0.0751]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha.forward(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4e511d-9b8f-466f-91e0-83129799d721",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
